{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Engineering_III_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QI1OIBvAFce"
      },
      "source": [
        "Train your own word2vec representations, as you did in the first example in this checkpoint. However, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUtEXBEs5zNY",
        "outputId": "a3f1a526-9ea5-4476-c474-9ddce9824535"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import gensim\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcISHArI6HSB"
      },
      "source": [
        "# apply all the text cleaning and model preparation\n",
        "\n",
        "# Utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
        "    # recognize: the double dash --. Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Load and clean the data\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# The chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)\n",
        "\n",
        "# Parse the cleaned novels. This can take some time.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)\n",
        "\n",
        "# Group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# Combine the sentences from the two novels into one DataFrame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "\n",
        "# Get rid of stop words and punctuation,\n",
        "# and lemmatize the tokens\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VNEYS5oBihQ"
      },
      "source": [
        "# train word2vec on the sentences\n",
        "model_1 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=4,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "# let's adjust the window size for another model\n",
        "model_2 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=10,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "# now let's also change the size\n",
        "model_3 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=10,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=300,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy24tVPIC1zF",
        "outputId": "cbe62123-831a-470b-912b-b752f9e8869e"
      },
      "source": [
        "# now prepare the model DataFrames\n",
        "word2vec_arr_1 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr_2 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr_3 = np.zeros((sentences.shape[0],300))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "  word2vec_arr_1[i,:] = np.mean([model_1[lemma] for lemma in sentence], axis=0)\n",
        "  word2vec_arr_2[i,:] = np.mean([model_2[lemma] for lemma in sentence], axis=0)\n",
        "  word2vec_arr_3[i,:] = np.mean([model_3[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr_1 = pd.DataFrame(word2vec_arr_1)\n",
        "word2vec_arr_2 = pd.DataFrame(word2vec_arr_2)\n",
        "word2vec_arr_3 = pd.DataFrame(word2vec_arr_3)\n",
        "\n",
        "sentences_1 = pd.concat([sentences[[\"author\", \"text\"]], word2vec_arr_1], axis=1)\n",
        "sentences_1.dropna(inplace=True)\n",
        "\n",
        "sentences_2 = pd.concat([sentences[[\"author\", \"text\"]], word2vec_arr_2], axis=1)\n",
        "sentences_2.dropna(inplace=True)\n",
        "\n",
        "sentences_3 = pd.concat([sentences[[\"author\", \"text\"]], word2vec_arr_3], axis=1)\n",
        "sentences_3.dropna(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoH_n39BFC4w",
        "outputId": "d03fa285-1113-4ffa-cf83-cdc7fcf545d6"
      },
      "source": [
        "# now for the actual model training and evaluation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y_1 = sentences_1[\"author\"]\n",
        "Y_2 = sentences_2[\"author\"]\n",
        "Y_3 = sentences_3[\"author\"]\n",
        "\n",
        "X_1 = np.array(sentences_1.drop([\"text\", \"author\"], 1))\n",
        "X_2 = np.array(sentences_2.drop([\"text\", \"author\"], 1))\n",
        "X_3 = np.array(sentences_3.drop([\"text\", \"author\"], 1))\n",
        "\n",
        "# splitting into training and test sets\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, Y_1, test_size=0.3, random_state=70)\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, Y_2, test_size=0.3, random_state=70)\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, Y_3, test_size=0.3, random_state=70)\n",
        "\n",
        "# load the models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "# fit the models and evaluate them\n",
        "print(\"Model 1\")\n",
        "lr.fit(X_train_1, y_train_1)\n",
        "rfc.fit(X_train_1, y_train_1)\n",
        "gbc.fit(X_train_1, y_train_1)\n",
        "print(\"LR Scores\")\n",
        "print(\"Training: \", lr.score(X_train_1, y_train_1))\n",
        "print(\"Test: \", lr.score(X_test_1, y_test_1))\n",
        "print(\"\\n\")\n",
        "print(\"RFC Scores\")\n",
        "print(\"Training: \", rfc.score(X_train_1, y_train_1))\n",
        "print(\"Test: \", rfc.score(X_test_1, y_test_1))\n",
        "print(\"\\n\")\n",
        "print(\"GBC Scores\")\n",
        "print(\"Training: \", gbc.score(X_train_1, y_train_1))\n",
        "print(\"Test: \", gbc.score(X_test_1, y_test_1))\n",
        "\n",
        "print(\"-------------------\")\n",
        "\n",
        "print(\"Model 2\")\n",
        "lr.fit(X_train_2, y_train_2)\n",
        "rfc.fit(X_train_2, y_train_2)\n",
        "gbc.fit(X_train_2, y_train_2)\n",
        "print(\"LR Scores\")\n",
        "print(\"Training: \", lr.score(X_train_2, y_train_2))\n",
        "print(\"Test: \", lr.score(X_test_2, y_test_2))\n",
        "print(\"\\n\")\n",
        "print(\"RFC Scores\")\n",
        "print(\"Training: \", rfc.score(X_train_2, y_train_2))\n",
        "print(\"Test: \", rfc.score(X_test_2, y_test_2))\n",
        "print(\"\\n\")\n",
        "print(\"GBC Scores\")\n",
        "print(\"Training: \", gbc.score(X_train_2, y_train_2))\n",
        "print(\"Test: \", gbc.score(X_test_2, y_test_2))\n",
        "\n",
        "print(\"-------------------\")\n",
        "\n",
        "print(\"Model 3\")\n",
        "lr.fit(X_train_3, y_train_3)\n",
        "rfc.fit(X_train_3, y_train_3)\n",
        "gbc.fit(X_train_3, y_train_3)\n",
        "print(\"LR Scores\")\n",
        "print(\"Training: \", lr.score(X_train_3, y_train_3))\n",
        "print(\"Test: \", lr.score(X_test_3, y_test_3))\n",
        "print(\"\\n\")\n",
        "print(\"RFC Scores\")\n",
        "print(\"Training: \", rfc.score(X_train_3, y_train_3))\n",
        "print(\"Test: \", rfc.score(X_test_3, y_test_3))\n",
        "print(\"\\n\")\n",
        "print(\"GBC Scores\")\n",
        "print(\"Training: \", gbc.score(X_train_3, y_train_3))\n",
        "print(\"Test: \", gbc.score(X_test_3, y_test_3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1\n",
            "LR Scores\n",
            "Training:  0.7860125260960334\n",
            "Test:  0.7845404747413268\n",
            "\n",
            "\n",
            "RFC Scores\n",
            "Training:  0.9929540709812108\n",
            "Test:  0.8174071819841753\n",
            "\n",
            "\n",
            "GBC Scores\n",
            "Training:  0.8885699373695198\n",
            "Test:  0.8119293974437005\n",
            "-------------------\n",
            "Model 2\n",
            "LR Scores\n",
            "Training:  0.8014091858037579\n",
            "Test:  0.7985392574558734\n",
            "\n",
            "\n",
            "RFC Scores\n",
            "Training:  0.9929540709812108\n",
            "Test:  0.8210590383444918\n",
            "\n",
            "\n",
            "GBC Scores\n",
            "Training:  0.8919624217118998\n",
            "Test:  0.8241022519780888\n",
            "-------------------\n",
            "Model 3\n",
            "LR Scores\n",
            "Training:  0.7682672233820459\n",
            "Test:  0.7735849056603774\n",
            "\n",
            "\n",
            "RFC Scores\n",
            "Training:  0.9929540709812108\n",
            "Test:  0.8167985392574558\n",
            "\n",
            "\n",
            "GBC Scores\n",
            "Training:  0.901356993736952\n",
            "Test:  0.8253195374315276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVcrm0vJIQ7n"
      },
      "source": [
        "The 2nd model gives slightly better results when compared to the model implemented in the checkpoint notebook, but it still shows signs of overfitting. It is better though, but it could be further improved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCehubCOHfHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}